---
title: "DSII HW1"
output: github_document
---

```{r message = FALSE}
library(tidyverse)
library(ISLR)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(pls)
```

```{r message = FALSE}
# setups

test = read_csv("./data/solubility_test.csv") %>%
  janitor::clean_names() %>% 
  na.omit()

train = read.csv("./data/solubility_train.csv") %>%
  janitor::clean_names() %>% 
  na.omit()

xtrain = model.matrix(solubility ~., train)[,-1]
ytrain = train$solubility

```

# Question 1: Linear model
```{r}
set.seed(1)

lm_fit = lm(solubility ~., data = train)
final_lm = predict(lm_fit, newdata = test)
mse_lm = mean((final_lm - test$solubility)^2)
mse_lm

```

MSE of the linear model fitting on the training data is `r mse_lm`.

# Question 2: Ridge regression model
```{r}
set.seed(1)

# Cross Validation
cv.ridge = cv.glmnet(xtrain, ytrain, 
                     type.measure = "mse",
                     alpha = 0,
                     lambda = exp(seq(-10, 5, length = 1000)))
plot(cv.ridge)
cv.ridge$lambda.min # min MSE
cv.ridge$lambda.1se # 1se
ridge = glmnet(xtrain, ytrain, 
               standardize = TRUE,
               alpha = 0,
               lambda = cv.ridge$lambda.min)
final_ridge = predict(ridge, newx = model.matrix(solubility ~., test)[,-1], s = cv.ridge$lambda.min, type = "response")
mse_ridge = mean((final_ridge - test$solubility)^2)
mse_ridge
```

MSE of Ridge model is `r mse_ridge`.

# Question 3: Lasso Model
```{r}
set.seed(1)

cv.lasso = cv.glmnet(xtrain, ytrain, 
                     type.measure = "mse",
                     alpha = 1,
                     lambda = exp(seq(-10, 3, length = 1000)))
plot(cv.lasso)
cv.lasso$lambda.min # min MSE
cv.lasso$lambda.1se # 1se
lasso = glmnet(xtrain, ytrain, 
               standardize = TRUE,
               alpha = 1,
               lambda = cv.lasso$lambda.min)
final_lasso = predict(lasso, newx = model.matrix(solubility ~., test)[,-1], s = cv.lasso$lambda.min, type = "response")
mse_lasso = mean((final_lasso - test$solubility)^2)
mse_lasso
sum(final_lasso!=0) # non-zero coefficients
```

MSE of Lasso Regression is `r mse_lasso`. The number of non-zero coefficient estimates is `r sum(final_lasso!=0)`.

# Question 4: Principle Component Regression Model
```{r}
set.seed(1)
pcr = pcr(solubility ~., data = train, scale = TRUE, validation = "CV")
summary(pcr) # n of components = 228
validationplot(pcr, val.type = "MSEP")
final_pcr = predict(pcr, newdata = test, ncomp = 228)
mse_pcr = mean((final_pcr - test$solubility)^2)
mse_pcr
```

MSE of PCR is `r mse_pcr`, wit M selected as 228.

# Question 5
```{r}
data.frame(mse_lm, mse_ridge, mse_lasso, mse_pcr)
```

To make the results comparable, MSE is selected to be the criteria of test error. Based on the table above, Lasso model has the least MSE, therefore, Lasso is preferred to predict solubility.








